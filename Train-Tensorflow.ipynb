{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b8f8e2-e1ed-43b5-8da0-deb433e76c46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ray\n",
    "\n",
    "from ray import tune\n",
    "from ray import serve\n",
    "from ray.air.config import ScalingConfig\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.train.xgboost import XGBoostPredictor\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray.serve import PredictorDeployment\n",
    "from ray.serve.http_adapters import pandas_read_json\n",
    "from ray.tune import Tuner, TuneConfig\n",
    "\n",
    "import requests\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5d3cba-e02a-4b85-aed0-d4d3a335d3e9",
   "metadata": {},
   "source": [
    "# Ray Train\n",
    "\n",
    "## Intro\n",
    "\n",
    "### Outline\n",
    "\n",
    "-   Goals\n",
    "-   Trainer\n",
    "    - Design\n",
    "    - Flavors\n",
    "    - In-depth with TensorFlow Trainer\n",
    "\n",
    "### Model scenarios with Ray + Tensorflow Trainer\n",
    "\n",
    "- Start with a minimal model and focus on key elements for Ray Train workflow\n",
    "- Port a minimal word2vec model from training locally in TF/Keras to Ray Train\n",
    "\n",
    "### Context: Ray AIR\n",
    "\n",
    "Ray AIR is the Ray AI Runtime, a set of high-level easy-to-use APIs for\n",
    "ingesting data, training models – including reinforcement learning\n",
    "models – tuning those models and then serving them.\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Introduction_to_Ray_AIR/e2e_air.png\" width=600 loading=\"lazy\"/>\n",
    "\n",
    "Key principles behind Ray and Ray AIR are\n",
    "* Performance\n",
    "* Developer experience and simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa6a9b-e702-45d2-b59a-75609738077c",
   "metadata": {},
   "source": [
    "__Read, preprocess with Ray Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9351ec6-88cc-4ab1-ae1b-ed025d9046bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = ray.data.read_parquet(\"s3://anyscale-training-data/intro-to-ray-air/nyc_taxi_2021.parquet\")\n",
    "\n",
    "train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffefcdb9-360b-4244-828a-8a49adb6a8a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "__Fit model with Ray Train__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1291d94e-376f-423c-b624-f1f6651e4dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = XGBoostTrainer(\n",
    "    label_column=\"is_big_tip\",\n",
    "    scaling_config=ScalingConfig(num_workers=32, use_gpu=False),\n",
    "    params={ \"objective\": \"binary:logistic\", },\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    ")\n",
    "\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6126df-8f78-4b4d-a955-a12b0b96d371",
   "metadata": {},
   "source": [
    "__Optimize hyperparams with Ray Tune__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fb7f4f-d803-40ee-8e7d-6655a396f950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner = Tuner(trainer, \n",
    "            param_space={'params' : {'max_depth': tune.randint(2, 12)}},\n",
    "            tune_config=TuneConfig(num_samples=10, metric='train-logloss', mode='min'))\n",
    "\n",
    "checkpoint = tuner.fit().get_best_result().checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e681533b-0df1-45c9-81bc-74e40ce076d8",
   "metadata": {},
   "source": [
    "__Batch prediction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1d33dc-d7d2-4a7f-b309-0b7aac030994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_predictor = BatchPredictor.from_checkpoint(checkpoint, XGBoostPredictor)\n",
    "\n",
    "predicted_probabilities = batch_predictor.predict(valid_dataset.drop_columns(['is_big_tip']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d66b61a-f001-483c-846c-02b5f8c06acc",
   "metadata": {},
   "source": [
    "__Online prediction with Ray Serve__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af460fe-18f9-4af0-8301-9d00ca00bca6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deployment = PredictorDeployment.bind(XGBoostPredictor, result.checkpoint, http_adapter=pandas_read_json)\n",
    "\n",
    "serve.run(deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592bab94-4fd1-41fc-a038-f77669d15922",
   "metadata": {},
   "source": [
    "__HTTP or Python services__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05578ff7-161b-4e5a-b825-18ceba9393f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_input = dict(valid_dataset.take(1)[0])\n",
    "del(sample_input['is_big_tip'])\n",
    "del(sample_input['__index_level_0__'])\n",
    "requests.post(\"http://localhost:8000/\", json=[sample_input]).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec926284-e57d-4ac8-a2ce-151e0e153dce",
   "metadata": {},
   "source": [
    "## Train Goals\n",
    "\n",
    "* Developer experience\n",
    "* Flexibility\n",
    "* Performance and simplicity via delegation\n",
    "    * Train does not re-implement distributed optimizers\n",
    "    * Train coordinates and delegates native platform distributed training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71b7829-aa50-45ee-8f5c-123d09d17572",
   "metadata": {},
   "source": [
    "## `Trainer` design and usage\n",
    "\n",
    "### Idea: Trainer -> Checkpoint\n",
    "   \n",
    "* Trainer used by Train, Tune\n",
    "* Checkpoint used for inference (Ray Data [batch], Serve [online]) and reporting\n",
    "\n",
    "### Trainer Flavors\n",
    "\n",
    "* Tree - e.g., XGBoost\n",
    "* Library - e.g., Huggingface\n",
    "* DL Trainers\n",
    "    * PyTorch, TensorFlow, Horovod, Lightning, Accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d515f2f-7e53-424e-a010-8e290e51e27f",
   "metadata": {},
   "source": [
    "### Focus: Tensorflow Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85894e8-86b3-46b0-a8ef-897ceadaf400",
   "metadata": {},
   "source": [
    "\"Hello World\" (iris) example with minimal model to look at data/train structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead525b-ea0a-4982-bfd2-ef513abdd0b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from ray.air import session\n",
    "from ray.air.integrations.keras import ReportCheckpointCallback\n",
    "from ray.train.tensorflow import TensorflowTrainer\n",
    "from ray.air.config import ScalingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a093b-83df-4a6b-bb25-59e4f95bd87c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = ray.data.read_csv(\"s3://air-example-data/iris.csv\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ab100-441e-4e11-8298-c33c4c8bafe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b858d09-a244-449b-828f-6ba02746b975",
   "metadata": {},
   "source": [
    "\"If your dataset contains multiple features but your model accepts a single tensor as input, combine features with Concatenator.\"\n",
    "https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.to_tf.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb72a71-998e-4db2-9eb6-db7fd4f8c7e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.data.preprocessors import Concatenator\n",
    "\n",
    "preprocessor = Concatenator(output_column_name=\"features\", exclude=\"target\")\n",
    "\n",
    "ds = preprocessor.transform(ds)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ef9474-33f9-48d2-8cb7-7ef7946e1bbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model() -> tf.keras.Model:\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.InputLayer(input_shape=(4,)),\n",
    "            tf.keras.layers.Dense(5),\n",
    "            tf.keras.layers.Dense(1),\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268ba098-1b28-4dc0-a627-af0951edd4aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Train func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90f816b-fd69-4161-bf09-c43d98460457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_func(config: dict):\n",
    "    batch_size = config.get(\"batch_size\", 64)\n",
    "    epochs = config.get(\"epochs\", 3)\n",
    "\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "    with strategy.scope():\n",
    "        # Model building/compiling need to be within `strategy.scope()`.\n",
    "        multi_worker_model = build_model()\n",
    "        multi_worker_model.compile(\n",
    "            optimizer=tf.keras.optimizers.SGD(learning_rate=config.get(\"lr\", 1e-3)),\n",
    "            loss=tf.keras.losses.mean_squared_error,\n",
    "            metrics=[tf.keras.metrics.mean_squared_error],\n",
    "        )\n",
    "\n",
    "    dataset = session.get_dataset_shard(\"train\")\n",
    "\n",
    "    results = []\n",
    "    for _ in range(epochs):\n",
    "        tf_dataset = dataset.to_tf(\n",
    "            feature_columns=\"features\", label_columns=\"target\", batch_size=batch_size\n",
    "        )\n",
    "        history = multi_worker_model.fit(\n",
    "            tf_dataset, callbacks=[ReportCheckpointCallback()]\n",
    "        )\n",
    "        results.append(history.history)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c2e2c0-b704-494f-97b3-397e826247e8",
   "metadata": {},
   "source": [
    "<img src='https://docs.ray.io/en/latest/_images/session.svg' width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8faee0-fa3d-432b-8435-b9ee5bed2581",
   "metadata": {},
   "source": [
    "* https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy\n",
    "* details\n",
    "    * https://docs.ray.io/en/latest/ray-air/api/session.html\n",
    "    * ray.air.integrations.keras.ReportCheckpointCallback https://docs.ray.io/en/latest/tune/api/doc/ray.air.integrations.keras.ReportCheckpointCallback.html\n",
    "    * \"To save a model to use for the TensorflowPredictor, you must save it under the “model” kwarg in Checkpoint passed to session.report().\"\n",
    "        * https://docs.ray.io/en/latest/train/api/doc/ray.train.tensorflow.TensorflowTrainer.html#ray.train.tensorflow.TensorflowTrainer\n",
    "\n",
    "manual checkpoint\n",
    "\n",
    "`checkpoint = Checkpoint.from_dict(dict(epoch=epoch, model_weights=model.get_weights()))` \n",
    "https://docs.ray.io/en/latest/train/dl_guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7502a6e-fdcb-46ea-9271-d514a8f32cc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_config = {\"lr\": 1e-3, \"batch_size\": 32, \"epochs\": 4}\n",
    "\n",
    "scaling_config = ScalingConfig(num_workers=2, use_gpu=False)\n",
    "\n",
    "trainer = TensorflowTrainer(\n",
    "    train_loop_per_worker=train_func,\n",
    "    train_loop_config=train_config,\n",
    "    scaling_config=scaling_config,\n",
    "    datasets={\"train\": ds},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5bc5bb-12e5-41cd-87b1-4028841ec3dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad20af9-34e5-41c5-a420-424171d15865",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a689919-2ae0-4ae9-8a60-48c52ca3c5d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result.checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7b4f7f-cc2e-4052-a28a-1e1be5f1b0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb50f048-8304-4249-9c0a-b53a4960bd59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96734b09-9c35-48bb-90ca-14acd7887635",
   "metadata": {},
   "source": [
    "Training dataset from Tensorflow word2vec tutorial (https://www.tensorflow.org/tutorials/text/word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4698f3-f10a-4e87-a275-e02dd5af6087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.load('w2v.data.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a563c-b2e7-4ef9-926e-5a1ce70d4dea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906f9997-d8e8-45c1-942c-59f6bae35a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "\n",
    "        num_ns = 4 # from dataset construction\n",
    "        self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=num_ns+1)\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "        # context: (batch, context)\n",
    "        if len(target.shape) == 2:\n",
    "            target = tf.squeeze(target, axis=1)\n",
    "        # target: (batch,)\n",
    "        word_emb = self.target_embedding(target)\n",
    "        # word_emb: (batch, embed)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        # context_emb: (batch, context, embed)\n",
    "        dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "        # dots: (batch, context)\n",
    "        return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa60e82-6a23-4173-824a-b1bb0aecfadd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = 4096\n",
    "embedding_dim = 128\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02f6641-3e1f-4239-803e-f4d9f4ecddcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "word2vec.fit(dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62cf971-d2b8-45ea-9b93-d6bc8ffa6f20",
   "metadata": {
    "tags": []
   },
   "source": [
    "https://docs.ray.io/en/latest/ray-air/examples/convert_existing_tf_code_to_ray_air.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988d9fb2-faee-4244-b3ae-8b8fbe5cbcd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Pass in the hyperparameter config\n",
    "def train_func(config: dict):\n",
    "    epochs = config.get(\"epochs\", 5)\n",
    "    batch_size_per_worker = config.get(\"batch_size\", 32)\n",
    "    buffer_size = config.get(\"buffer_size\", 8192)\n",
    "    \n",
    "    # 2. Synchronized model setup\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "    with strategy.scope():\n",
    "        vocab_size = 4096\n",
    "        embedding_dim = 128\n",
    "        model = Word2Vec(vocab_size, embedding_dim)\n",
    "        model.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    # 3. Shard the dataset across `session.get_world_size()` workers\n",
    "    global_batch_size = batch_size_per_worker * session.get_world_size()\n",
    "    \n",
    "    ds_path = config.get('tf_data') # if we're using classic TF Data, this must be globally accessible\n",
    "    train_ds = tf.data.Dataset.load(ds_path).shuffle(buffer_size).batch(global_batch_size).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    if session.get_world_rank() == 0:\n",
    "        print(f\"\\nDataset is sharded across {session.get_world_size()} workers:\")\n",
    "        # The number of samples is approximate, because is not always\n",
    "        # a multiple of batch_size, so some batches could contain fewer than\n",
    "        # `batch_size_per_worker` samples.\n",
    "        print(\n",
    "            f\"# training batches per worker = {len(train_ds)} \"\n",
    "            f\"(~{len(train_ds) * batch_size_per_worker} samples)\"\n",
    "        )\n",
    "  \n",
    "    # 4. Report metrics and checkpoint the model\n",
    "    report_metrics_and_checkpoint_callback = ReportCheckpointCallback(report_metrics_on=\"epoch_end\")\n",
    "    model.fit(\n",
    "        train_ds,\n",
    "        batch_size=batch_size_per_worker,\n",
    "        epochs=epochs,\n",
    "        callbacks=[report_metrics_and_checkpoint_callback],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47721bc5-4181-4429-96ed-ef6f33ecbea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_config = {\"batch_size\": BATCH_SIZE, \"epochs\": 4, \"buffer_size\" : BUFFER_SIZE, \"tf_data\" : os.path.abspath('w2v.data.tf')}\n",
    "\n",
    "scaling_config = ScalingConfig(num_workers=8, use_gpu=False)\n",
    "\n",
    "trainer = TensorflowTrainer(\n",
    "    train_loop_per_worker=train_func,\n",
    "    train_loop_config=train_config,\n",
    "    scaling_config=scaling_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f0f9d0-0c35-41ea-bf2e-db7f0a8ff5ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68dd839-d563-413b-9905-5053f96aef80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
