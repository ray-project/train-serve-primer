{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4392476c-6bcc-4caa-85b8-16e87bedf99d",
   "metadata": {},
   "source": [
    "# Anyscale Services HA and Canary Rollout Features\n",
    "\n",
    "## High availability for web services and ML serving\n",
    "\n",
    "High availability (HA) refers to the ability of a system to continue to function despite one or more component failures.\n",
    "\n",
    "Traditional web service\n",
    "* Load balancers or layer 7 switches are placed as the external front end to traffic\n",
    "    * Load is distributed across multiple service instances\n",
    "    * Additional capacity and the ability to add service instances provides HA\n",
    "* Stateful services complicate this picture a bit\n",
    "    * Data storage must also be HA; this involves at least temporary compromises to ensure soundness\n",
    "\n",
    "Scale-out clustered compute service for ML\n",
    "* Modern machine learning workloads often leverage scale-out clustering technologies like Ray\n",
    "* Cluster compute environments typically do *not* offer HA for performance and architecture reasons\n",
    "* __Anyscale services__ enable HA for Ray Serve\n",
    "    * *No single point of failure: even Ray's head node can fail without impacting service availability or capacity*\n",
    "    * For the user of Anyscale services, this is available by default and requires no configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec270573-80a9-4edc-ba65-f69b5886d95e",
   "metadata": {},
   "source": [
    "<img src='images/grafana-dash.png' width=800 />\n",
    "\n",
    "## Canary service rollouts\n",
    "\n",
    "The canary rollout feature allows zero-downtime upgrades as a live service transitions from one implementation to a new one\n",
    "* Additional clusters are automatically provisioned by Anyscale for new service versions\n",
    "    * service versions do *not* need to share config, dependencies, or even hardware requirements\n",
    "    * the only thing that stays the same is the (internal) name and external endpoint\n",
    "* Load is gradually shifted from the old service to the new one by Anyscale load balancers\n",
    "    * rollout (changeover) schedule can be automatic, customized, or manually controlled\n",
    "* Status of old and new versions are visible and accessible simultaneously in the Anyscale UI\n",
    "    * Grafana integration shows realtime statistics on service transition\n",
    "    * __Rollback__ feature is available if it is necessary to abort the transition and return all traffic to original service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e544086-94ac-4ff9-8cbe-98db34d207df",
   "metadata": {},
   "source": [
    "### Demo of canary rollouts\n",
    "\n",
    "We'll demonstrate rolling out multiple service versions while monitoring both externally and via Anyscale\n",
    "\n",
    "#### Setup\n",
    "\n",
    "The service versions are implemented in Python using standard Ray Serve APIs\n",
    "* `1-hello-world.py` - simple \"hello world\" echo service\n",
    "* `2-hello-chat.py` - skeleton for a chat service, it generates a response in a trivial static manner\n",
    "* `3-llm-chat.py` - functional chatbot service built on Huggingface and the `blenderbot-400M-distill model`\n",
    "\n",
    "Each service version has a corresponding YAML file used to deploy that version -- 1-service.yaml, 2-service.yaml, etc. Extended configuration is possible via these YAML files as well as via CLI parameters, but the present examples are minimal starting points for clarity.\n",
    "\n",
    "Prior to running the present notebook, a security token was obtained and stored in `token.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad6f29-eeff-451e-a4b8-ca7aa1e01880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('token.txt', 'r') as f:\n",
    "    token = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b1b365-ad31-4019-a119-f59e6c91f8bc",
   "metadata": {},
   "source": [
    "Each Anyscale service has a unique URL -- calls to this URL will be routed automatically during the version changeover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b381127-c27a-499d-bf5b-0782329ba61b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_url = \"https://service-demo-xa4v3.cld-kvedzwag2qa8i5bj.s.anyscaleuserdata.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca31e942-ec33-4f40-9f3b-01708cfa6099",
   "metadata": {},
   "source": [
    "We'll set up minimal code to make a request to our service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75996c5-bc66-4562-884b-b0cefdcd9e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "path = \"/\"\n",
    "full_url = f\"{base_url}{path}\"\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a324ea-d85b-4724-9665-7e2c9f172cc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_json = '{ \"user_input\" : \"hello\", \"history\":[] }'\n",
    "\n",
    "requests.post(base_url,  headers={'Authorization': 'Bearer '+ token }, json = sample_json).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c2b0f3-ebb7-42d3-8884-35e1caa872b8",
   "metadata": {},
   "source": [
    "Currently, the service is not deployed\n",
    "\n",
    "#### Initial service rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca11884-1261-45eb-a0fe-73a478b0a04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! anyscale service rollout -f 1-service.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2db29c-9c5c-4fc9-bd38-b406e0294d0c",
   "metadata": {},
   "source": [
    "In the Anyscale UI (or via logs) we can monitor the initial rollout\n",
    "\n",
    "> We can launch `load_test.py` in a console to generate a steady stream of requests to our service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce76b864-f80e-4df2-949b-490c0ac508d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "requests.post(base_url,  headers={'Authorization': 'Bearer '+ token }, json = sample_json).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc41227-3bd1-4d21-a15b-6b3890dd74e1",
   "metadata": {},
   "source": [
    "#### Upgrading the service\n",
    "\n",
    "When we are ready to upgrade the service, we issue another CLI command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48724c7b-2362-40f2-9b37-ddeeefc255c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! anyscale service rollout -f 2-service.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8151c88-424a-4bc8-ae91-d08f2aa8cf04",
   "metadata": {},
   "source": [
    "At this point, we may want to observe the canary rollout service changeover in\n",
    "* the Anyscale service UI\n",
    "* Grafana timeseries chart of all-version traffic\n",
    "* service response changes to local requests (visible by running `tail -f data.txt` to view the output stats from the `load_test.py` script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f6f4f7-48cd-45d0-ac36-665d2428be02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "requests.post(base_url,  headers={'Authorization': 'Bearer '+ token }, json = sample_json).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e394ac-d566-45a3-b550-06d0f4387c0f",
   "metadata": {},
   "source": [
    "#### LLM chat service\n",
    "\n",
    "To change over to a real LLM-backed chat service, we run another similar CLI command\n",
    "\n",
    "Note that\n",
    "* real LLM-backed service requires different software (Python libraries) and hardware (GPU)\n",
    "    * those changes are managed automatically with no new config\n",
    "* LLM chat service, due to model inference computations, involves more latency than the \"hello world\" service\n",
    "    * capacity/throughput of the LLM chat model can be improved by adding more replicas or autoscaling that service deployment (using the standard Ray Serve APIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66db4b7f-e596-4cf2-b1f7-1e25cca5d8bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! anyscale service rollout -f 3-service.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2c96d2-7d22-436a-8fa5-ca97ded1abe9",
   "metadata": {},
   "source": [
    "We can further manage the service via Anyscale UI, Python SDK or CLI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
